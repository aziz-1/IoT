{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# --- Step 1: Load ---\n",
        "import pickle\n",
        "\n",
        "# Load normalized graph and metadata\n",
        "with open(\"/content/mmb_graph_enhanced.gpickle\", \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "with open(\"/content/node_to_chunks_enhanced.json\") as f:\n",
        "    node_to_chunks = json.load(f)\n",
        "\n",
        "with open(\"/content/node_to_community_enhanced.json\") as f:\n",
        "    node_to_community = json.load(f)\n",
        "\n",
        "# with open(\"/content/chunk_metadata.json\") as f:\n",
        "#     chunk_metadata = json.load(f)"
      ],
      "metadata": {
        "id": "dvQRHCSP698W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from collections import defaultdict, Counter\n",
        "from rapidfuzz import fuzz\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import networkx as nx\n",
        "from collections import defaultdict, Counter\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "### 1. Normalize node names ###\n",
        "def normalize_node_name(name):\n",
        "    name = re.sub(r'^[\\d\\.\\-\\(\\)\\s]+', '', name)  # Remove leading numbers/symbols\n",
        "    name = name.strip().lower()\n",
        "    return name\n",
        "\n",
        "\n",
        "### 2. Normalize existing graph nodes and update node_to_chunks ###\n",
        "def normalize_graph(G, node_to_chunks):\n",
        "    # Build mapping from original to normalized names\n",
        "    mapping = {node: normalize_node_name(node) for node in G.nodes}\n",
        "\n",
        "    # Relabel graph nodes safely\n",
        "    G_normalized = nx.relabel_nodes(G, mapping, copy=True)\n",
        "\n",
        "    # Rebuild node_to_chunks with normalized keys\n",
        "    node_to_chunks_norm = defaultdict(set)\n",
        "    for node, chunks in node_to_chunks.items():\n",
        "        new_node = normalize_node_name(node)\n",
        "        node_to_chunks_norm[new_node].update(chunks)\n",
        "\n",
        "    return G_normalized, node_to_chunks_norm\n",
        "\n",
        "\n",
        "### 3. Fuzzy merge similar nodes (with shared chunk evidence) ###\n",
        "def build_merge_map(G, node_to_chunks, chunk_metadata, threshold=87):\n",
        "    nodes = list(G.nodes)\n",
        "    merge_map = {}\n",
        "    reverse_map = {}\n",
        "\n",
        "    for i in range(len(nodes)):\n",
        "        for j in range(i + 1, len(nodes)):\n",
        "            n1, n2 = nodes[i], nodes[j]\n",
        "            sim = fuzz.token_sort_ratio(n1, n2)\n",
        "            if sim >= threshold:\n",
        "                shared_chunks = node_to_chunks.get(n1, set()) & node_to_chunks.get(n2, set())\n",
        "                if len(shared_chunks) >= 1:\n",
        "                    rep = min(n1, n2, key=len)\n",
        "                    merge_map[n1] = rep\n",
        "                    merge_map[n2] = rep\n",
        "                    reverse_map.setdefault(rep, set()).update([n1, n2])\n",
        "    return merge_map, reverse_map\n",
        "\n",
        "\n",
        "### 4. Apply node merging to graph and mappings ###\n",
        "def apply_merge(G, node_to_chunks, merge_map):\n",
        "    G_cleaned = nx.relabel_nodes(G, merge_map, copy=True)\n",
        "\n",
        "    node_to_chunks_cleaned = defaultdict(set)\n",
        "    for node, chunks in node_to_chunks.items():\n",
        "        new_node = merge_map.get(node, node)\n",
        "        node_to_chunks_cleaned[new_node].update(chunks)\n",
        "\n",
        "    return G_cleaned, node_to_chunks_cleaned\n",
        "\n",
        "def entropy(items):\n",
        "    freq = np.array(list(Counter(items).values()))\n",
        "    probs = freq / freq.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXP420q38FZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_community(node_to_community, node_to_chunks, chunk_metadata):\n",
        "    comm_entropy = {}\n",
        "    community_chunks = defaultdict(set)\n",
        "    for node, comm_id in node_to_community.items():\n",
        "        community_chunks[comm_id].update(node_to_chunks.get(node, []))\n",
        "\n",
        "    for comm_id, chunk_ids in community_chunks.items():\n",
        "        carriers = [chunk_metadata[cid]['metadata']['carrier']\n",
        "            for cid in chunk_ids\n",
        "            if cid in chunk_metadata and 'metadata' in chunk_metadata[cid] and 'carrier' in chunk_metadata[cid]['metadata']]\n",
        "        if carriers:\n",
        "            comm_entropy[comm_id] = entropy(carriers)\n",
        "    return comm_entropy"
      ],
      "metadata": {
        "id": "Od1el5WC8P6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Normalize first\n",
        "G_normalized, node_to_chunks_norm = normalize_graph(G, node_to_chunks)\n",
        "\n",
        "# STEP 2: Fuzzy match + build merge map\n",
        "merge_map, reverse_map = build_merge_map(G_normalized, node_to_chunks_norm, chunk_metadata)\n",
        "\n",
        "# STEP 3: Apply merge to graph and chunk map\n",
        "G_cleaned, node_to_chunks_cleaned = apply_merge(G_normalized, node_to_chunks_norm, merge_map)\n",
        "\n",
        "# STEP 4: Optional community detection\n",
        "from community import community_louvain\n",
        "node_to_community_cleaned = community_louvain.best_partition(G_cleaned)"
      ],
      "metadata": {
        "id": "T9cjZNg-8SNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_embedding(chunk_id, chunk_metadata):\n",
        "    record = chunk_metadata.get(chunk_id, {})\n",
        "    return record.get(\"metadata\",{}).get(\"embedding\", None)\n",
        "\n",
        "def build_node_features(node_to_chunks, chunk_metadata):\n",
        "    node_features = {}\n",
        "    for node, chunk_ids in node_to_chunks.items():\n",
        "        embs = [get_embedding(cid, chunk_metadata) for cid in chunk_ids if get_embedding(cid, chunk_metadata)]\n",
        "        if embs:\n",
        "            node_features[node] = np.mean(embs, axis=0)\n",
        "    return node_features\n"
      ],
      "metadata": {
        "id": "D0rNOs498USN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_node_labels(node_to_chunks, chunk_metadata):\n",
        "    node_labels = {}\n",
        "    for node, chunk_ids in node_to_chunks.items():\n",
        "        categories = [\n",
        "            chunk_metadata[cid].get(\"metadata\", {}).get(\"category\")\n",
        "            for cid in chunk_ids\n",
        "            if \"metadata\" in chunk_metadata[cid] and \"category\" in chunk_metadata[cid][\"metadata\"]\n",
        "        ]\n",
        "        if categories:\n",
        "            node_labels[node] = Counter(categories).most_common(1)[0][0]\n",
        "    return node_labels"
      ],
      "metadata": {
        "id": "CkCd14Ia8Vrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "def prepare_pyg_data(G, node_features, node_labels):\n",
        "    node_to_id = {node: idx for idx, node in enumerate(node_features)}\n",
        "    x = torch.tensor([node_features[node] for node in node_to_id], dtype=torch.float)\n",
        "    unique_labels = sorted(set(node_labels.values()))\n",
        "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    y = torch.full((len(node_to_id),), -1, dtype=torch.long)\n",
        "    for node, label in node_labels.items():\n",
        "        if node in node_to_id:\n",
        "            y[node_to_id[node]] = label_to_id[label]\n",
        "    train_mask = y != -1\n",
        "\n",
        "    # Build edge_index and edge_type\n",
        "    edge_list = []\n",
        "    edge_types = []\n",
        "    relation_map = {}\n",
        "    rel_counter = 0\n",
        "    for u, v, d in G.edges(data=True):\n",
        "        edge_list.append([node_to_id[u], node_to_id[v]])\n",
        "        rel = d.get('relation', 'default')\n",
        "        if rel not in relation_map:\n",
        "            relation_map[rel] = rel_counter\n",
        "            rel_counter += 1\n",
        "        edge_types.append(relation_map[rel])\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "    edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, edge_type=edge_type)\n",
        "    return data, label_to_id, relation_map"
      ],
      "metadata": {
        "id": "bpTajukY8W03"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}